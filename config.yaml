model:
  primary: "oai_4o_latest"           # String for the main LLM model to use
  allow_image_input: true    # Boolean to enable image inputs into the model

graph_verification:
  enabled: true              # Boolean to enable graph verification
  method: "llm"              # Options: "llm" (requires model) or "bert"
  llm_model: "microsoft_3.8b_phi3"         # Only used if method is "llm"
  # A light weight model that can handle JSON is required for processing graph chunks
  # 3.8 Billion 

prompt_mode:
  mode: "elaborated"         # Options: "original", "elaborated", "q_learning", "q_training"
  elaborator_model: "oai_4o_latest"  # String for the LLM model to use for prompt elaboration
# 


text_summariser:
  model: "llava_13b"     # String for the LLM model to use for text summarization
  # This is for converting images to text so needs to be a multimodal model but also light enough to have fast processing.


embedding:
  method: "aws"        # Options: "langchain", "clip", or "aws"
  model: "amazon.titan-embed-image-v1"     # Required if method is "langchain" or "aws"

logger:
  level: "INFO"              # Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"